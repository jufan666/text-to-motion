# GRPO 训练指标说明

## 指标含义

### 1. **loss** (总损失)
- **含义**: GRPO 的总损失值，用于反向传播更新模型
- **公式**: `loss = -(policy_loss - β * kl_penalty)`
- **正常范围**: 通常在 `[-10, 10]` 之间，取决于 KL 惩罚权重
- **当前值**: `171551552.0000` ❌ **异常！**

### 2. **policy_loss** (策略损失)
- **含义**: PPO 风格的策略损失，衡量策略改进
- **公式**: `policy_loss = mean(min(ratio * A, clipped_ratio * A))`
- **正常范围**: 通常在 `[-1, 1]` 之间
- **当前值**: `171531792.0000` ❌ **异常！**

### 3. **kl_penalty** (KL 散度惩罚)
- **含义**: 当前模型与参考模型的 KL 散度，用于防止策略偏离太远
- **公式**: `kl = log_prob_current - log_prob_ref`
- **正常范围**: 通常在 `[-1, 1]` 之间（取决于 KL 惩罚权重）
- **当前值**: `197562.5312` ❌ **异常！**

### 4. **mean_ratio** (平均比率)
- **含义**: 当前模型与参考模型的概率比率
- **公式**: `ratio = exp(log_prob_current - log_prob_ref)`
- **正常范围**: 应该在 `[0.5, 2.0]` 附近（接近 1.0）
- **当前值**: `485165216.0000` ❌ **严重异常！**

### 5. **mean_advantage** (平均优势)
- **含义**: 组内相对优势的平均值
- **公式**: `A_i = (r_i - mean(r_group)) / (std(r_group) + ε)`
- **正常范围**: 应该在 `[-1, 1]` 之间（因为组内归一化）
- **当前值**: `0.0000` ⚠️ **异常！**（说明组内奖励没有差异）

### 6. **mean_log_prob_current** (当前模型平均 log 概率)
- **含义**: 当前模型对生成轨迹的平均 log probability
- **正常范围**: 
  - 对于低维数据：通常在 `[-1000, -100]` 之间
  - 对于高维数据（如 263*1*196*50步）：可能在 `[-1e6, 1e6]` 范围内
  - **关键**：log prob 的绝对值大本身不是问题，关键是 log_prob_current 和 log_prob_ref 的**差值**应该在合理范围内（< 10）
- **当前值**: `-833610.3125` ⚠️ **绝对值大但可能正常**（取决于数据维度）

### 7. **mean_log_prob_ref** (参考模型平均 log 概率)
- **含义**: 参考模型对相同轨迹的平均 log probability
- **正常范围**: 与 log_prob_current 相同
- **当前值**: `-1031172.8750` ⚠️ **绝对值大但可能正常**

### 8. **mean_reward** (平均奖励)
- **含义**: 生成动作的平均奖励值
- **正常范围**: 通常在 `[0, 1]` 之间（如果奖励函数已归一化）
- **当前值**: `0.4971` ✅ **正常**

### 9. **std_reward** (奖励标准差)
- **含义**: 奖励值的标准差，衡量奖励的分散程度
- **正常范围**: 取决于奖励函数，通常在 `[0, 0.5]` 之间
- **当前值**: `0.1766` ✅ **正常**

## 异常分析

### 🔴 严重异常指标

1. **mean_ratio: 485165216** 
   - **问题**: ratio 应该接近 1.0，当前值异常大
   - **原因**: `log_prob_current - log_prob_ref = 197562`，导致 `exp(197562)` 溢出
   - **影响**: 导致 policy_loss 和 loss 异常大
   - **修复**: 代码已添加 `log_ratio` 限制（-10 到 10），ratio 限制（1e-4 到 1e4）

2. **mean_log_prob_current 和 mean_log_prob_ref 绝对值大**
   - **问题**: log prob 的绝对值很大（几十万到几百万）
   - **原因分析**:
     - **正常情况**: 对于高维数据（263 关节 * 1 特征 * 196 帧 * 50 时间步），累积 log prob 的绝对值确实会很大
     - **异常情况**: 如果 log_prob_current 和 log_prob_ref 的**差值**很大（> 10），说明模型偏离太远
   - **判断标准**: 
     - ✅ 如果 `|log_prob_current - log_prob_ref| < 10`，即使绝对值大也是正常的
     - ❌ 如果 `|log_prob_current - log_prob_ref| > 10`，说明模型异常
   - **当前状态**: 从警告信息看，差值达到 -94037，说明模型已经偏离参考模型太远

3. **mean_advantage: 0.0000**
   - **问题**: 优势为 0 说明组内所有样本的奖励相同
   - **原因**: 
     - 奖励函数可能没有区分度
     - 或者组内样本确实相同（不应该发生）

### ⚠️ 可能的原因

1. **模型偏离参考模型太远**
   - **主要问题**: `log_prob_current - log_prob_ref` 差值过大（-94037）
   - **原因**: 
     - 学习率可能过大，导致模型更新太快
     - KL 惩罚可能太小，无法有效限制模型偏离
     - 模型可能已经训练了太多步，偏离了初始状态

2. **Log Probability 计算问题**
   - log prob 的绝对值过大是**正常的**（对于高维数据）
   - 但差值过大说明模型状态异常
   - 可能的原因：
     - 模型输出的均值和方差异常
     - 采样轨迹与模型预测不匹配

3. **数值稳定性问题**
   - 虽然已经添加了 clamp，但 log_ratio 的原始值太大（-94037）
   - 需要更严格的限制和更早的干预

## 解决方案

### 立即措施

1. **停止训练**：当前训练已经不稳定，继续训练可能损坏模型

2. **检查模型状态**：
   - 确认参考模型是否正确加载（应该是冻结的预训练模型）
   - 确认当前模型是否使用了 LoRA（应该只训练 LoRA 参数）
   - 检查模型参数是否正常

3. **降低学习率**（最重要）：
   ```bash
   --learning_rate 1e-6  # 从 1e-5 降低到 1e-6，甚至 5e-7
   ```

4. **增加 KL 惩罚**（很重要）：
   ```bash
   --kl_penalty 1.0  # 从 0.1 增加到 1.0，更严格地限制策略偏离
   ```

5. **减小裁剪范围**：
   ```bash
   --clip_epsilon 0.1  # 从 0.2 减小到 0.1，更保守的更新
   ```

6. **从检查点恢复**（如果可用）：
   - 如果之前有保存的检查点，从更早的检查点恢复
   - 或者重新开始训练，使用更保守的参数

### 长期修复

1. **改进 log prob 计算**：
   - 检查是否需要对 log prob 进行归一化
   - 检查轨迹长度是否合理

2. **更严格的数值限制**：
   - 进一步限制 log_ratio 的范围
   - 添加梯度裁剪

3. **监控训练状态**：
   - 添加更多检查点
   - 监控模型参数的变化

## 正常训练指标示例

```
loss: 0.1234
policy_loss: 0.8567
kl_penalty: 0.0045
mean_ratio: 1.0234
mean_advantage: 0.0123
mean_log_prob_current: -256.78
mean_log_prob_ref: -258.45
mean_reward: 0.6234
std_reward: 0.1234
```

## 建议的训练参数

如果遇到数值不稳定（log_ratio 差值 > 10），建议：

```bash
--learning_rate 5e-7  # 非常小的学习率（从 1e-5 降低）
--kl_penalty 1.0      # 更大的 KL 惩罚（从 0.1 增加）
--clip_epsilon 0.1    # 更小的裁剪范围（从 0.2 减小）
--group_size 2        # 更小的组大小（减少计算复杂度）
--batch_size 1        # 更小的批次大小（减少内存和计算）
```

### 训练参数调整策略

1. **如果 log_ratio 差值在 [5, 10]**：
   - 降低学习率到 `1e-6`
   - 增加 KL 惩罚到 `0.5`

2. **如果 log_ratio 差值在 [10, 20]**：
   - 降低学习率到 `5e-7`
   - 增加 KL 惩罚到 `1.0`
   - 减小 clip_epsilon 到 `0.1`

3. **如果 log_ratio 差值 > 20**：
   - **停止训练**，检查模型状态
   - 从检查点恢复或重新开始
   - 使用非常保守的参数：`--learning_rate 1e-7 --kl_penalty 2.0`

