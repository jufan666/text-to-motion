# GRPO (Group Relative Policy Optimization) for Motion Diffusion Models

## æ¦‚è¿°

GRPO æ˜¯ä¸€ç§æ— éœ€ Critic ç½‘ç»œçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œé€‚ç”¨äºå¾®è°ƒæ–‡æœ¬ç”ŸæˆåŠ¨ä½œçš„æ‰©æ•£æ¨¡å‹ã€‚å®ƒé€šè¿‡ç»„å†…ç›¸å¯¹ä¼˜åŠ¿æ¥æ›¿ä»£ä¼ ç»Ÿçš„ç»å¯¹ä¼˜åŠ¿ä¼°è®¡ï¼Œä»è€Œé¿å…äº†ä»·å€¼å‡½æ•°ï¼ˆCriticï¼‰çš„éœ€æ±‚ï¼Œå¤§å¹…é™ä½æ˜¾å­˜å ç”¨ã€‚

## æ ¸å¿ƒæ€æƒ³

### 1. ç»„é‡‡æ · (Group Sampling)
å¯¹äºæ¯ä¸ªæ–‡æœ¬æç¤º $x$ï¼Œæˆ‘ä»¬ç”Ÿæˆ $G$ ä¸ªæ ·æœ¬ï¼ˆç»„å¤§å°ï¼Œä¾‹å¦‚ $G=4$ æˆ– $8$ï¼‰ï¼š
- ä»å½“å‰ç­–ç•¥ $\pi_\theta$ é‡‡æ ·åŠ¨ä½œåºåˆ— $\{y_1, y_2, ..., y_G\}$
- æ‰€æœ‰æ ·æœ¬ä½¿ç”¨ç›¸åŒçš„åˆå§‹å™ªå£°å’Œæ¡ä»¶ï¼Œç¡®ä¿å¯æ¯”è¾ƒæ€§

### 2. å¥–åŠ±è®¡ç®— (Reward Computation)
ä½¿ç”¨å¥–åŠ±æ¨¡å‹ï¼ˆReward Modelï¼‰è®¡ç®—æ¯ä¸ªç”ŸæˆåŠ¨ä½œçš„å¥–åŠ±ï¼š
- **å¥–åŠ±æ¨¡å‹** (`reward_model.py`): åŸºäº MDM è¯„ä¼°å™¨ï¼Œè®¡ç®—æ–‡æœ¬-åŠ¨ä½œåŒ¹é…åˆ†æ•°
- å¯¹äºæ¯ä¸ªç”Ÿæˆçš„åŠ¨ä½œ $y_i$ï¼Œå¥–åŠ±æ¨¡å‹è¾“å‡ºæ ‡é‡å¥–åŠ± $r_i$
- å¥–åŠ±å€¼é€šå¸¸å½’ä¸€åŒ–åˆ° [0, 1] èŒƒå›´

### 3. ä¼˜åŠ¿ä¼°è®¡ (Advantage Estimation)
åˆ©ç”¨ç»„å†…ç»Ÿè®¡è®¡ç®—ç›¸å¯¹ä¼˜åŠ¿ï¼Œæ— éœ€ Criticï¼š
$$A_i = \frac{r_i - \text{mean}(\mathbf{r})}{\text{std}(\mathbf{r}) + \epsilon}$$

**å…³é”®**ï¼šä¼˜åŠ¿ä¸æ˜¯ç”± Critic ç½‘ç»œè®¡ç®—çš„ï¼Œè€Œæ˜¯é€šè¿‡ç»„å†…å½’ä¸€åŒ–å¾—åˆ°çš„ç›¸å¯¹ä¼˜åŠ¿ã€‚

### 4. ç›®æ ‡å‡½æ•°
$$\mathcal{L}_{\text{GRPO}} = \frac{1}{G} \sum_{i=1}^G \left[ \min \left( \text{ratio}_i \cdot A_i, \text{clip}(\text{ratio}_i, 1-\epsilon, 1+\epsilon) \cdot A_i \right) - \beta D_{KL}(\pi_\theta || \pi_{\text{ref}}) \right]$$

å…¶ä¸­ $\text{ratio}_i = \frac{\pi_\theta(y_i|x)}{\pi_{\text{ref}}(y_i|x)}$ã€‚

## è®­ç»ƒæµç¨‹è¯¦è§£

### å®Œæ•´è®­ç»ƒæ­¥éª¤

1. **Rollout é˜¶æ®µ**ï¼šä½¿ç”¨å½“å‰æ¨¡å‹ä¸ºæ¯ä¸ª prompt ç”Ÿæˆ $G$ ä¸ªæ ·æœ¬
2. **Log Prob è®¡ç®—**ï¼šè®¡ç®—å½“å‰æ¨¡å‹å’Œå‚è€ƒæ¨¡å‹å¯¹ç”Ÿæˆè½¨è¿¹çš„ log probability
3. **å¥–åŠ±è®¡ç®—**ï¼šä½¿ç”¨å¥–åŠ±æ¨¡å‹è®¡ç®—æ¯ä¸ªç”ŸæˆåŠ¨ä½œçš„å¥–åŠ±å€¼
4. **ä¼˜åŠ¿è®¡ç®—**ï¼šå°†å¥–åŠ±è½¬æ¢ä¸ºç»„å†…ç›¸å¯¹ä¼˜åŠ¿
5. **æŸå¤±è®¡ç®—**ï¼šä½¿ç”¨ä¼˜åŠ¿ã€ratio å’Œ KL æƒ©ç½šè®¡ç®— GRPO æŸå¤±
6. **æ¢¯åº¦æ›´æ–°**ï¼šåå‘ä¼ æ’­å¹¶æ›´æ–°æ¨¡å‹å‚æ•°ï¼ˆä»… LoRA å‚æ•°ï¼‰

### Log Probability è®¡ç®—ï¼ˆåŸºäº DanceGRPO è®ºæ–‡ï¼‰

æ ¹æ® DanceGRPO è®ºæ–‡çš„å…¬å¼ (7) å’Œ (8)ï¼Œç­–ç•¥ $\pi_\theta(z_{t-1}|z_t)$ è¢«å»ºæ¨¡ä¸ºé«˜æ–¯åˆ†å¸ƒã€‚

**æ ¸å¿ƒæ–¹æ³• `get_batch_log_prob`**ï¼š

1. **è½¨è¿¹ä¿å­˜**ï¼šåœ¨é‡‡æ ·è¿‡ç¨‹ä¸­ä¿å­˜å®Œæ•´è½¨è¿¹ `[z_T, z_{T-1}, ..., z_0]`
2. **é«˜æ–¯ Log Prob è®¡ç®—**ï¼šå¯¹äºæ¯ä¸ªæ—¶é—´æ­¥ $t$ï¼š
   - æ¨¡å‹é¢„æµ‹å‡å€¼ $\mu_\theta$ å’Œæ–¹å·® $\sigma^2$
   - å®é™…é‡‡æ ·ç‚¹ $z_{t-1}$ åœ¨ $N(\mu_\theta, \sigma^2)$ ä¸‹çš„ log probabilityï¼š
     $$\log p(z_{t-1}) = -\frac{1}{2}\left(\frac{z_{t-1} - \mu_\theta}{\sigma}\right)^2 - \log(\sigma) - \frac{1}{2}\log(2\pi)$$
3. **ç´¯ç§¯æ±‚å’Œ**ï¼šå¯¹æ‰€æœ‰æ—¶é—´æ­¥çš„ log prob æ±‚å’Œï¼Œå¾—åˆ°æ•´æ¡è½¨è¿¹çš„ç´¯ç§¯ log probability

**å…³é”®ä¼˜åŠ¿**ï¼š
- ç›´æ¥è®¡ç®—è¿ç»­ç©ºé—´çš„é«˜æ–¯æ¦‚ç‡ï¼Œæ— éœ€ç¦»æ•£åŒ–
- ä½¿ç”¨å®é™…é‡‡æ ·è½¨è¿¹ï¼Œè®¡ç®—æ›´å‡†ç¡®
- æ”¯æŒæ¢¯åº¦å›ä¼ ï¼Œå¯ç”¨äºç­–ç•¥ä¼˜åŒ–

### ç»„é‡‡æ ·å¤„ç†

- æ¯ä¸ª prompt é‡å¤ $G$ æ¬¡ï¼Œå½¢æˆ `batch_size * group_size` çš„æ‰©å±•æ‰¹æ¬¡
- ä½¿ç”¨ç›¸åŒçš„å™ªå£°ç§å­ç¡®ä¿å¯æ¯”è¾ƒæ€§
- ç»„å†…ä¼˜åŠ¿å½’ä¸€åŒ–ç¡®ä¿è®­ç»ƒç¨³å®šæ€§

## é¡¹ç›®ç»“æ„

GRPO ç›¸å…³ä»£ç ä½äº `model/GRPO/` ç›®å½•ä¸‹ï¼š

```
model/
  GRPO/
    __init__.py          # æ¨¡å—å¯¼å‡º
    grpo_trainer.py      # GRPO è®­ç»ƒå™¨æ ¸å¿ƒå®ç°
    reward_model.py      # åŸºäº MDM è¯„ä¼°å™¨çš„å¥–åŠ±æ¨¡å‹
```

### ä½¿ç”¨ MDM è¯„ä¼°å™¨ä½œä¸ºå¥–åŠ±æ¨¡å‹

é¡¹ç›®æä¾›äº†åŸºäº MDM è¯„ä¼°å™¨çš„å¥–åŠ±å‡½æ•°å®ç°ï¼ˆ`model/GRPO/reward_model.py`ï¼‰ï¼š

```python
from model.GRPO.reward_model import create_mdm_reward_function

# åˆ›å»ºåŸºäºåŒ¹é…åˆ†æ•°çš„å¥–åŠ±å‡½æ•°
reward_fn = create_mdm_reward_function(
    reward_type='matching',  # 'matching', 'r_precision', æˆ– 'combined'
    dataset_name='humanml',
    device='cuda',
)

# ä½¿ç”¨
rewards = reward_fn(motions, prompts)  # [B*G]
```

**å¯ç”¨çš„å¥–åŠ±ç±»å‹**ï¼š
- `'matching'`: åŸºäºæ–‡æœ¬-åŠ¨ä½œåŒ¹é…åˆ†æ•°ï¼ˆæ¬§æ°è·ç¦»ï¼‰
- `'r_precision'`: åŸºäº R-Precision æ£€ç´¢ç²¾åº¦
- `'combined'`: ç»„åˆå¤šç§æŒ‡æ ‡

#### ä¸ºä»€ä¹ˆåªä½¿ç”¨ Matching Score å’Œ R-Precisionï¼Ÿ

MDM é¡¹ç›®å®šä¹‰äº† 5 ç§è¯„ä¼°æŒ‡æ ‡ï¼Œä½†åªæœ‰éƒ¨åˆ†é€‚åˆä½œä¸º GRPO çš„å¥–åŠ±å‡½æ•°ï¼š

| è¯„ä¼°æŒ‡æ ‡ | æ˜¯å¦é€‚åˆä½œä¸ºå¥–åŠ± | åŸå›  |
|---------|----------------|------|
| **Matching Score** | âœ… **é€‚åˆ** | å¯ä»¥ä¸ºæ¯ä¸ªæ ·æœ¬å•ç‹¬è®¡ç®—ï¼Œç›´æ¥è¡¡é‡æ–‡æœ¬-åŠ¨ä½œåŒ¹é…åº¦ |
| **R-Precision** | âœ… **é€‚åˆ** | å¯ä»¥ä¸ºæ¯ä¸ªæ ·æœ¬å•ç‹¬è®¡ç®—ï¼Œè¡¡é‡æ£€ç´¢ç²¾åº¦ |
| **FID** | âŒ **ä¸é€‚åˆ** | éœ€è¦è®¡ç®—æ•´ä¸ªæ•°æ®é›†çš„ç»Ÿè®¡ç‰¹å¾ï¼ˆå‡å€¼å’Œåæ–¹å·®ï¼‰ï¼Œæ˜¯æ‰¹é‡æŒ‡æ ‡ï¼Œä¸èƒ½ä¸ºå•ä¸ªæ ·æœ¬è®¡ç®— |
| **Diversity** | âŒ **ä¸é€‚åˆ** | éœ€è¦å¤šä¸ªæ ·æœ¬æ‰èƒ½è®¡ç®—å¤šæ ·æ€§ï¼Œæ˜¯æ‰¹é‡æŒ‡æ ‡ |
| **MultiModality** | âŒ **ä¸é€‚åˆ** | éœ€è¦ä¸ºæ¯ä¸ªæ–‡æœ¬ç”Ÿæˆå¤šä¸ªæ ·æœ¬ï¼Œç„¶åè®¡ç®—æ ·æœ¬é—´å·®å¼‚ï¼Œæ˜¯æ‰¹é‡æŒ‡æ ‡ |

**GRPO å¯¹å¥–åŠ±å‡½æ•°çš„è¦æ±‚**ï¼š
1. **å•æ ·æœ¬è®¡ç®—**ï¼šå¿…é¡»èƒ½ä¸ºæ¯ä¸ªç”Ÿæˆçš„åŠ¨ä½œåºåˆ— $y_i$ è®¡ç®—ä¸€ä¸ªæ ‡é‡å¥–åŠ± $r_i$
2. **è´¨é‡åæ˜ **ï¼šå¥–åŠ±åº”è¯¥åæ˜ è¯¥æ ·æœ¬çš„è´¨é‡ï¼ˆç‰¹åˆ«æ˜¯ä¸æ–‡æœ¬çš„åŒ¹é…åº¦ï¼‰
3. **æ¢¯åº¦å‹å¥½**ï¼šå¥–åŠ±å€¼åº”è¯¥èƒ½å¤ŸæŒ‡å¯¼ç­–ç•¥ä¼˜åŒ–ï¼ˆé€šè¿‡ä¼˜åŠ¿å‡½æ•°ï¼‰

**æ¨èä½¿ç”¨**ï¼š
- **Matching Score**ï¼ˆæ¨èï¼‰ï¼šæœ€ç›´æ¥ã€æœ€å¸¸ç”¨ï¼Œè®¡ç®—æ•ˆç‡é«˜ï¼Œç›´æ¥åæ˜ æ–‡æœ¬-åŠ¨ä½œå¯¹é½è´¨é‡
- **R-Precision**ï¼šå¯ä»¥ä½œä¸ºè¾…åŠ©æŒ‡æ ‡ï¼Œä½†è®¡ç®—ç›¸å¯¹å¤æ‚
- **Combined**ï¼šç»„åˆ Matching Score å’Œ R-Precisionï¼Œå¯èƒ½è·å¾—æ›´å¥½çš„æ•ˆæœ

**æ³¨æ„**ï¼šFIDã€Diversity å’Œ MultiModality è™½ç„¶ä¸é€‚åˆä½œä¸ºå•ä¸ªæ ·æœ¬çš„å¥–åŠ±ï¼Œä½†å®ƒä»¬ä»ç„¶å¯ä»¥ä½œä¸º**è®­ç»ƒåçš„è¯„ä¼°æŒ‡æ ‡**ï¼Œç”¨äºè¯„ä¼°æ•´ä½“æ¨¡å‹æ€§èƒ½ã€‚

## å‚æ•°è¯´æ˜

### GRPOTrainer å‚æ•°

- `group_size` (int, default=4): æ¯ä¸ª prompt çš„é‡‡æ ·æ•°é‡ $G$
- `clip_epsilon` (float, default=0.2): PPO é£æ ¼çš„è£å‰ªå‚æ•°
- `kl_penalty` (float, default=0.1): KL æ•£åº¦æƒ©ç½šæƒé‡ $\beta$
- `advantage_eps` (float, default=1e-8): ä¼˜åŠ¿å½’ä¸€åŒ–çš„æ•°å€¼ç¨³å®šæ€§å‚æ•°
- `use_checkpointing` (bool, default=False): æ˜¯å¦ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹èŠ‚çœæ˜¾å­˜

### è®­ç»ƒå‚æ•°å»ºè®®

- **Group Size**: 4-8 é€šå¸¸æ•ˆæœè¾ƒå¥½ã€‚æ›´å¤§çš„ç»„å¯ä»¥æä¾›æ›´ç¨³å®šçš„ä¼˜åŠ¿ä¼°è®¡ï¼Œä½†ä¼šå¢åŠ è®¡ç®—æˆæœ¬
- **Learning Rate**: 1e-5 åˆ° 1e-4ï¼Œé€šå¸¸æ¯”æ ‡å‡†è®­ç»ƒæ›´å°
- **KL Penalty**: 0.01-0.1ï¼Œæ§åˆ¶ç­–ç•¥åç¦»å‚è€ƒæ¨¡å‹çš„ç¨‹åº¦
- **Batch Size**: æ ¹æ®æ˜¾å­˜è°ƒæ•´ï¼Œæ³¨æ„å®é™…æ‰¹æ¬¡å¤§å°æ˜¯ `batch_size * group_size`

## æ˜¾å­˜ä¼˜åŒ–

1. **ä½¿ç”¨ LoRA**ï¼šåªè®­ç»ƒä½ç§©é€‚é…å™¨ï¼Œå¤§å¹…å‡å°‘å¯è®­ç»ƒå‚æ•°
2. **æ¢¯åº¦æ£€æŸ¥ç‚¹**ï¼šè®¾ç½® `use_checkpointing=True`
3. **å‡å°‘ç»„å¤§å°**ï¼šè¾ƒå°çš„ $G$ å¯ä»¥å‡å°‘æ˜¾å­˜å ç”¨
4. **æ··åˆç²¾åº¦è®­ç»ƒ**ï¼šå¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–ï¼ˆéœ€è¦é¢å¤–å®ç°ï¼‰

## æ³¨æ„äº‹é¡¹

1. **Log Probability è®¡ç®—**ï¼šå½“å‰å®ç°ä½¿ç”¨è½¨è¿¹é‡å»ºæ–¹æ³•ï¼Œå¯¹äºéå¸¸é•¿çš„åºåˆ—å¯èƒ½ä¸å¤Ÿç²¾ç¡®ã€‚å¯¹äºæ›´ç²¾ç¡®çš„è®¡ç®—ï¼Œéœ€è¦è·Ÿè¸ªå®é™…çš„é‡‡æ ·è½¨è¿¹ã€‚

2. **å¥–åŠ±å‡½æ•°è®¾è®¡**ï¼šå¥–åŠ±å‡½æ•°çš„è´¨é‡ç›´æ¥å½±å“è®­ç»ƒæ•ˆæœã€‚ç¡®ä¿å¥–åŠ±å‡½æ•°ï¼š
   - èƒ½å¤ŸåŒºåˆ†å¥½æ ·æœ¬å’Œåæ ·æœ¬
   - æ•°å€¼èŒƒå›´åˆç†ï¼ˆå»ºè®®å½’ä¸€åŒ–åˆ° [0, 1] æˆ– [-1, 1]ï¼‰
   - è®¡ç®—æ•ˆç‡é«˜ï¼ˆä¼šè¢«é¢‘ç¹è°ƒç”¨ï¼‰

3. **ç»„å¤§å°é€‰æ‹©**ï¼š
   - å¤ªå°çš„ $G$ï¼ˆå¦‚ 2ï¼‰å¯èƒ½å¯¼è‡´ä¼˜åŠ¿ä¼°è®¡ä¸ç¨³å®š
   - å¤ªå¤§çš„ $G$ï¼ˆå¦‚ 16+ï¼‰ä¼šå¢åŠ è®¡ç®—æˆæœ¬ï¼Œä½†å¯èƒ½ä¸ä¼šå¸¦æ¥æ˜¾è‘—æå‡

4. **KL æƒ©ç½š**ï¼š
   - å¤ªå°çš„ $\beta$ å¯èƒ½å¯¼è‡´ç­–ç•¥åç¦»å‚è€ƒæ¨¡å‹å¤ªå¿«
   - å¤ªå¤§çš„ $\beta$ å¯èƒ½é™åˆ¶ç­–ç•¥æ”¹è¿›

## æ•…éšœæ’é™¤

### é—®é¢˜ 1: æ˜¾å­˜ä¸è¶³ (CUDA out of memory)

**é”™è¯¯ä¿¡æ¯ç¤ºä¾‹**ï¼š
```
torch.cuda.OutOfMemoryError: CUDA out of memory. 
Tried to allocate 20.00 MiB (GPU 0; 23.65 GiB total capacity; 
12.84 GiB already allocated; 17.56 MiB free)
```

**åŸå› åˆ†æ**ï¼š
GRPO è®­ç»ƒéœ€è¦ä¿å­˜å®Œæ•´çš„æ‰©æ•£è½¨è¿¹ï¼ˆæ‰€æœ‰æ—¶é—´æ­¥çš„ä¸­é—´çŠ¶æ€ï¼‰ï¼Œå†…å­˜å ç”¨éå¸¸å¤§ï¼š
- æ¯ä¸ªæ ·æœ¬éœ€è¦ä¿å­˜ `T` ä¸ªä¸­é—´çŠ¶æ€ï¼ˆT æ˜¯æ‰©æ•£æ­¥æ•°ï¼Œé€šå¸¸ 50-1000ï¼‰
- å®é™…æ‰¹æ¬¡å¤§å° = `batch_size * group_size`
- æ€»å†…å­˜ â‰ˆ `batch_size * group_size * T * latent_size`

**è§£å†³æ–¹æ¡ˆï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰**ï¼š

1. **å‡å°æ‰¹æ¬¡å¤§å°**ï¼ˆæœ€æœ‰æ•ˆï¼‰
   ```bash
   # å°† batch_size ä» 2 å‡å°åˆ° 1
   --batch_size 1
   
   # æˆ–å°† group_size ä» 4 å‡å°åˆ° 2
   --group_size 2
   ```

2. **å‡å°æ‰©æ•£æ­¥æ•°**ï¼ˆå¦‚æœä½¿ç”¨ respaceï¼‰
   - å¦‚æœä½¿ç”¨ `RespaceDiffusion`ï¼Œå¯ä»¥å‡å°‘å®é™…é‡‡æ ·æ­¥æ•°
   - æ³¨æ„ï¼šè¿™å¯èƒ½ä¼šå½±å“ç”Ÿæˆè´¨é‡

3. **ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹**
   ```python
   trainer = create_grpo_trainer(
       ...,
       use_checkpointing=True,  # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
   )
   ```

4. **ä½¿ç”¨ LoRA**ï¼ˆå‡å°‘æ¨¡å‹å‚æ•°ï¼‰
   ```bash
   --use_lora --lora_r 4 --lora_alpha 8  # ä½¿ç”¨æ›´å°çš„ LoRA å‚æ•°
   ```

5. **æ¸…ç† GPU ç¼“å­˜**
   - ä»£ç å·²è‡ªåŠ¨åœ¨è®¡ç®— log prob åæ¸…ç†è½¨è¿¹
   - å¦‚æœä»æœ‰é—®é¢˜ï¼Œå¯ä»¥åœ¨è®­ç»ƒå¾ªç¯ä¸­æ·»åŠ ï¼š
     ```python
     torch.cuda.empty_cache()
     ```

6. **åˆ†æ‰¹å¤„ç†**ï¼ˆé«˜çº§ï¼‰
   - å¦‚æœå¿…é¡»ä½¿ç”¨å¤§æ‰¹æ¬¡ï¼Œå¯ä»¥å®ç°åˆ†æ‰¹å¤„ç†é€»è¾‘
   - å°† `batch_size * group_size` åˆ†æˆå¤šä¸ªå°æ‰¹æ¬¡å¤„ç†

**æ¨èé…ç½®ï¼ˆ24GB GPUï¼‰**ï¼š
- `batch_size=1`, `group_size=4`ï¼ˆæ€»æ‰¹æ¬¡ 4ï¼‰
- `batch_size=2`, `group_size=2`ï¼ˆæ€»æ‰¹æ¬¡ 4ï¼‰
- ä½¿ç”¨ LoRA (`--use_lora`)

**æ¨èé…ç½®ï¼ˆ12GB GPUï¼‰**ï¼š
- `batch_size=1`, `group_size=2`ï¼ˆæ€»æ‰¹æ¬¡ 2ï¼‰
- å¿…é¡»ä½¿ç”¨ LoRA
- è€ƒè™‘å‡å°æ‰©æ•£æ­¥æ•°

### é—®é¢˜ 2: è®­ç»ƒä¸ç¨³å®š
- æ£€æŸ¥å¥–åŠ±å‡½æ•°æ˜¯å¦åˆç†
- è°ƒæ•´ `kl_penalty` å’Œ `clip_epsilon`
- å‡å°å­¦ä¹ ç‡

### é—®é¢˜ 3: Log Prob è®¡ç®—é”™è¯¯
- ç¡®ä¿ä½¿ç”¨ç›¸åŒçš„å™ªå£°ç§å­
- æ£€æŸ¥æ¨¡å‹è¾“å‡ºæ˜¯å¦åœ¨åˆç†èŒƒå›´å†…
- éªŒè¯æ‰©æ•£è¿‡ç¨‹çš„å‚æ•°è®¾ç½®

## ä»£ç ç»“æ„è¯´æ˜

### `model/GRPO/grpo_trainer.py`
- `GRPOTrainer`: GRPO è®­ç»ƒå™¨ä¸»ç±»
  - `get_batch_log_prob()`: è®¡ç®—æ‰¹é‡ log probabilityï¼ˆåŸºäº DanceGRPO è®ºæ–‡ï¼‰
  - `sample_with_trajectory()`: é‡‡æ ·å¹¶ä¿å­˜å®Œæ•´è½¨è¿¹
  - `compute_group_advantages()`: è®¡ç®—ç»„ç›¸å¯¹ä¼˜åŠ¿
  - `compute_grpo_loss()`: è®¡ç®— GRPO æŸå¤±
  - `step()`: æ‰§è¡Œä¸€æ­¥è®­ç»ƒ
- `create_grpo_trainer()`: å·¥å‚å‡½æ•°ï¼Œåˆ›å»º GRPO è®­ç»ƒå™¨

### `model/GRPO/reward_model.py`
- `MDMRewardFunction`: å¥–åŠ±å‡½æ•°åŸºç±»ï¼Œæä¾›æ–‡æœ¬å’ŒåŠ¨ä½œé¢„å¤„ç†åŠŸèƒ½
- `MatchingScoreReward`: åŸºäºåŒ¹é…åˆ†æ•°çš„å¥–åŠ±å‡½æ•°
  - ä½¿ç”¨ `EvaluatorMDMWrapper` è®¡ç®—æ–‡æœ¬å’ŒåŠ¨ä½œåµŒå…¥
  - é€šè¿‡æ¬§æ°è·ç¦»è¡¡é‡æ–‡æœ¬-åŠ¨ä½œåŒ¹é…åº¦
  - è·ç¦»è¶Šå°ï¼Œå¥–åŠ±è¶Šå¤§
- `RPrecisionReward`: åŸºäº R-Precision çš„å¥–åŠ±å‡½æ•°
  - è¡¡é‡åœ¨ top-k æ£€ç´¢ä¸­æ­£ç¡®åŒ¹é…çš„æ¯”ä¾‹
- `CombinedMDMReward`: ç»„åˆå¤šç§æŒ‡æ ‡çš„å¥–åŠ±å‡½æ•°
  - å¯ç»„åˆåŒ¹é…åˆ†æ•°å’Œ R-Precision
- `create_mdm_reward_function()`: å·¥å‚å‡½æ•°ï¼Œåˆ›å»ºå¥–åŠ±å‡½æ•°

**å¥–åŠ±æ¨¡å‹åœ¨è®­ç»ƒä¸­çš„ä½¿ç”¨**ï¼š
åœ¨ `GRPOTrainer.step()` æ–¹æ³•ä¸­ï¼Œå¥–åŠ±æ¨¡å‹è¢«è°ƒç”¨æ¥è®¡ç®—ç”ŸæˆåŠ¨ä½œçš„å¥–åŠ±ï¼š
```python
# åœ¨ grpo_trainer.py çš„ step() æ–¹æ³•ä¸­
rewards = self.reward_fn(motions, expanded_prompts)  # [B*G]
# ç„¶åè®¡ç®—ä¼˜åŠ¿
advantages = self.compute_group_advantages(rewards)  # [B*G]
# æœ€åç”¨äºè®¡ç®— GRPO loss
loss = compute_grpo_loss(log_prob_current, log_prob_ref, advantages)
```





# GRPO Reward å‡½æ•°è®¾è®¡åˆ†æ

## å½“å‰å®ç°æ¦‚è¿°

### 1. å¥–åŠ±å‡½æ•°æ¶æ„

GRPO çš„å¥–åŠ±å‡½æ•°åŸºäº **MDM è¯„ä¼°å™¨**ï¼ˆ`EvaluatorMDMWrapper`ï¼‰ï¼Œè¯¥è¯„ä¼°å™¨å°†æ–‡æœ¬å’ŒåŠ¨ä½œæ˜ å°„åˆ°å…±åŒçš„åµŒå…¥ç©ºé—´ã€‚

**æ ¸å¿ƒæµç¨‹**ï¼š

```
æ–‡æœ¬æç¤º â†’ æ–‡æœ¬åµŒå…¥ (512ç»´)
åŠ¨ä½œåºåˆ— â†’ åŠ¨ä½œåµŒå…¥ (512ç»´)
â†“
è®¡ç®—åµŒå…¥è·ç¦» â†’ è½¬æ¢ä¸ºå¥–åŠ±å€¼
```

### 2. ä¸‰ç§å¥–åŠ±å‡½æ•°å®ç°

#### (1) MatchingScoreRewardï¼ˆé»˜è®¤ï¼Œæœ€å¸¸ç”¨ï¼‰

**å®ç°é€»è¾‘**ï¼š

```python
# 1. è·å–æ–‡æœ¬å’ŒåŠ¨ä½œåµŒå…¥
text_embeddings, motion_embeddings = evaluator.get_co_embeddings(...)

# 2. è®¡ç®—æ¬§æ°è·ç¦»
distances = torch.norm(text_embeddings - motion_embeddings, dim=-1)  # [B]

# 3. çº¿æ€§å½’ä¸€åŒ–åˆ° [0, 1]
max_distance = 10.0  # ç¡¬ç¼–ç 
rewards = 1.0 - torch.clamp(distances / max_distance, 0, 1)
```

**ç‰¹ç‚¹**ï¼š

- âœ… ç®€å•ç›´æ¥ï¼Œè®¡ç®—æ•ˆç‡é«˜
- âœ… å¥–åŠ±èŒƒå›´å›ºå®šä¸º [0, 1]
- âŒ `max_distance=10.0` ç¡¬ç¼–ç ï¼Œå¯èƒ½ä¸é€‚åˆæ‰€æœ‰æƒ…å†µ
- âŒ è·ç¦» > 10 æ—¶å¥–åŠ±ä¸º 0ï¼Œå¯èƒ½ä¸¢å¤±ä¿¡æ¯

#### (2) RPrecisionReward

**å®ç°é€»è¾‘**ï¼š

```python
# è®¡ç®—è·ç¦»çŸ©é˜µ
dist_mat = euclidean_distance_matrix(text_emb, motion_emb)

# å¯¹äºæ¯ä¸ªæ ·æœ¬ï¼Œæ£€æŸ¥æ˜¯å¦åœ¨ top-k ä¸­
if i in top_k_indices:
    reward = 1.0
else:
    reward = 1.0 / (1.0 + distances[i])  # è·ç¦»å€’æ•°
```

**ç‰¹ç‚¹**ï¼š

- âœ… è€ƒè™‘äº†ç›¸å¯¹æ’å
- âŒ è®¡ç®—å¤æ‚åº¦è¾ƒé«˜ï¼ˆéœ€è¦è®¡ç®—è·ç¦»çŸ©é˜µï¼‰
- âŒ å¥–åŠ±åˆ†å¸ƒå¯èƒ½ä¸å¤Ÿå¹³æ»‘

#### (3) CombinedMDMReward

**å®ç°é€»è¾‘**ï¼š

```python
combined_rewards = (
    matching_weight * matching_rewards +
    r_precision_weight * r_precision_rewards
)
```

**ç‰¹ç‚¹**ï¼š

- âœ… ç»“åˆå¤šç§æŒ‡æ ‡ï¼Œå¯èƒ½æ›´å…¨é¢
- âŒ è®¡ç®—æˆæœ¬æ›´é«˜
- âŒ éœ€è¦è°ƒä¼˜æƒé‡

---

## âœ… è®¾è®¡åˆç†æ€§åˆ†æ

### ä¼˜ç‚¹

1. **åŸºäºæˆç†Ÿçš„è¯„ä¼°å™¨**
   - ä½¿ç”¨ MDM é¡¹ç›®å·²æœ‰çš„è¯„ä¼°å™¨ï¼Œç»è¿‡éªŒè¯
   - æ–‡æœ¬å’ŒåŠ¨ä½œåµŒå…¥åœ¨è”åˆç©ºé—´ä¸­ï¼Œè¯­ä¹‰å¯¹é½è‰¯å¥½

2. **å•æ ·æœ¬å¯è®¡ç®—**
   - æ»¡è¶³ GRPO çš„è¦æ±‚ï¼šå¯ä»¥ä¸ºæ¯ä¸ªæ ·æœ¬å•ç‹¬è®¡ç®—å¥–åŠ±
   - ä¸ä¾èµ–æ‰¹é‡ç»Ÿè®¡ï¼ˆå¦‚ FIDã€Diversityï¼‰

3. **æ•°å€¼èŒƒå›´åˆç†**
   - å¥–åŠ±å½’ä¸€åŒ–åˆ° [0, 1]ï¼Œä¾¿äºä¼˜åŠ¿è®¡ç®—
   - é¿å…äº†å¥–åŠ±å°ºåº¦é—®é¢˜

4. **ä¸è¯„ä¼°æŒ‡æ ‡ä¸€è‡´**
   - Matching Score æ˜¯ MDM è®ºæ–‡ä¸­çš„ä¸»è¦è¯„ä¼°æŒ‡æ ‡
   - å¥–åŠ±å‡½æ•°ä¸è¯„ä¼°æŒ‡æ ‡å¯¹é½ï¼Œè®­ç»ƒç›®æ ‡æ˜ç¡®

### æ½œåœ¨é—®é¢˜

#### ğŸ”´ é—®é¢˜ 1: ç¡¬ç¼–ç çš„ max_distance

**å½“å‰å®ç°**ï¼š

```python
max_distance = 10.0  # ç¡¬ç¼–ç 
rewards = 1.0 - torch.clamp(distances / max_distance, 0, 1)
```

**é—®é¢˜**ï¼š

- å¦‚æœå®é™…è·ç¦»åˆ†å¸ƒä¸åœ¨ [0, 10] èŒƒå›´å†…ï¼Œå¥–åŠ±ä¼šé¥±å’Œ
- ä¾‹å¦‚ï¼šå¦‚æœè·ç¦»é€šå¸¸åœ¨ [0, 5]ï¼Œé‚£ä¹ˆå¤§éƒ¨åˆ†å¥–åŠ±åœ¨ [0.5, 1.0]ï¼ŒåŒºåˆ†åº¦ä¸å¤Ÿ
- å¦‚æœè·ç¦»ç»å¸¸ > 10ï¼Œå¥–åŠ±ä¼šè¢«æˆªæ–­ä¸º 0ï¼Œä¸¢å¤±ä¿¡æ¯

**å»ºè®®æ”¹è¿›**ï¼š

```python
# æ–¹æ³• 1: è‡ªé€‚åº”å½’ä¸€åŒ–ï¼ˆåŸºäºå†å²ç»Ÿè®¡ï¼‰
# åœ¨è®­ç»ƒå¼€å§‹æ—¶ï¼Œé‡‡æ ·ä¸€æ‰¹æ ·æœ¬ï¼Œè®¡ç®—è·ç¦»åˆ†å¸ƒ
# ä½¿ç”¨åˆ†ä½æ•°ï¼ˆå¦‚ 95% åˆ†ä½æ•°ï¼‰ä½œä¸º max_distance

# æ–¹æ³• 2: ä½¿ç”¨æŒ‡æ•°è¡°å‡ï¼ˆæ›´å¹³æ»‘ï¼‰
scale = 2.0  # å¯è°ƒå‚æ•°
rewards = torch.exp(-distances / scale)

# æ–¹æ³• 3: ä½¿ç”¨åˆ†ä½æ•°å½’ä¸€åŒ–
# è®¡ç®—å½“å‰æ‰¹æ¬¡çš„è·ç¦»åˆ†ä½æ•°ï¼ŒåŠ¨æ€è°ƒæ•´
```

#### ğŸ”´ é—®é¢˜ 2: å¥–åŠ±åˆ†å¸ƒå¯èƒ½ä¸å¤Ÿæ•æ„Ÿ

**é—®é¢˜**ï¼š

- çº¿æ€§å½’ä¸€åŒ–å¯èƒ½å¯¼è‡´å¥–åŠ±åˆ†å¸ƒé›†ä¸­åœ¨æŸä¸ªèŒƒå›´
- å¯¹äºè·ç¦»å·®å¼‚å°çš„æ ·æœ¬ï¼Œå¥–åŠ±å·®å¼‚å¯èƒ½ä¸å¤Ÿæ˜æ˜¾

**å»ºè®®**ï¼š

- ä½¿ç”¨éçº¿æ€§å˜æ¢ï¼ˆå¦‚ sigmoidã€tanhï¼‰å¢å¼ºåŒºåˆ†åº¦
- æˆ–è€…ä½¿ç”¨æ’åå½’ä¸€åŒ–ï¼ˆrank normalizationï¼‰

#### ğŸ”´ é—®é¢˜ 3: æ²¡æœ‰è€ƒè™‘åŠ¨ä½œè´¨é‡çš„å…¶ä»–ç»´åº¦

**å½“å‰è®¾è®¡åªè€ƒè™‘æ–‡æœ¬-åŠ¨ä½œåŒ¹é…åº¦**ï¼Œä½†åŠ¨ä½œè´¨é‡è¿˜åŒ…æ‹¬ï¼š

- **æµç•…æ€§**ï¼šåŠ¨ä½œæ˜¯å¦è‡ªç„¶ã€è¿è´¯
- **å¤šæ ·æ€§**ï¼šåŠ¨ä½œæ˜¯å¦è¿‡äºå•è°ƒ
- **ç‰©ç†åˆç†æ€§**ï¼šæ˜¯å¦ç¬¦åˆç‰©ç†è§„å¾‹ï¼ˆå¦‚è¶³éƒ¨æ¥è§¦ï¼‰

**å»ºè®®**ï¼š

- å¯ä»¥æ·»åŠ é¢å¤–çš„å¥–åŠ±é¡¹ï¼ˆéœ€è¦é¢å¤–çš„è¯„ä¼°å™¨ï¼‰
- æˆ–è€…ä½¿ç”¨ç»„åˆå¥–åŠ±å‡½æ•°

#### ğŸ”´ é—®é¢˜ 4: RPrecisionReward çš„å®ç°å¯èƒ½æœ‰é—®é¢˜

**å½“å‰å®ç°**ï¼š

```python
for i in range(batch_size):
    distances = dist_mat[i]
    top_k_indices = np.argsort(distances)[:self.top_k]
    if i in top_k_indices:
        reward = 1.0
    else:
        reward = 1.0 / (1.0 + distances[i])
```

**é—®é¢˜**ï¼š

- åœ¨ GRPO ä¸­ï¼Œæ¯ä¸ª prompt ç”Ÿæˆ G ä¸ªæ ·æœ¬ï¼Œè¿™äº›æ ·æœ¬åº”è¯¥ä¸åŒä¸€ä¸ªæ–‡æœ¬æ¯”è¾ƒ
- ä½†å½“å‰å®ç°ä¸­ï¼Œ`dist_mat[i]` æ˜¯ç¬¬ i ä¸ªæ–‡æœ¬ä¸æ‰€æœ‰åŠ¨ä½œçš„è·ç¦»ï¼Œè¿™å¯èƒ½ä¸æ˜¯æˆ‘ä»¬æƒ³è¦çš„
- åº”è¯¥è®¡ç®—ï¼šå¯¹äºç¬¬ i ä¸ªæ–‡æœ¬ï¼Œå®ƒåœ¨æ‰€æœ‰åŠ¨ä½œä¸­çš„æ’å

**å»ºè®®ä¿®æ­£**ï¼š

```python
# å¯¹äºæ¯ä¸ªæ–‡æœ¬-åŠ¨ä½œå¯¹ï¼Œè®¡ç®—è¯¥åŠ¨ä½œåœ¨æ‰€æœ‰åŠ¨ä½œä¸­çš„æ’å
for i in range(batch_size):
    # è·å–ç¬¬ i ä¸ªæ–‡æœ¬å¯¹åº”çš„åŠ¨ä½œè·ç¦»
    distances = dist_mat[i]  # [batch_size]
    # è®¡ç®—æ’åï¼ˆè·ç¦»è¶Šå°ï¼Œæ’åè¶Šé å‰ï¼‰
    rank = (distances < distances[i]).sum()  # æœ‰å¤šå°‘ä¸ªåŠ¨ä½œè·ç¦»æ›´å°
    # å¦‚æœæ’ååœ¨ top-k ä¸­ï¼Œç»™äºˆé«˜å¥–åŠ±
    if rank < self.top_k:
        reward = 1.0
    else:
        # ä½¿ç”¨æ’åå€’æ•°ä½œä¸ºå¥–åŠ±
        reward = 1.0 / (1.0 + rank)
```

---

## ğŸ’¡ æ”¹è¿›å»ºè®®

### æ”¹è¿› 1: è‡ªé€‚åº”å½’ä¸€åŒ–

```python
class AdaptiveMatchingScoreReward(MDMRewardFunction):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.distance_history = []
        self.max_history_size = 1000
        self.percentile = 95  # ä½¿ç”¨ 95% åˆ†ä½æ•°
    
    def __call__(self, motions, prompts, lengths=None):
        # ... è®¡ç®—è·ç¦» ...
        distances = torch.norm(text_embeddings - motion_embeddings, dim=-1)
        
        # æ›´æ–°å†å²
        self.distance_history.extend(distances.cpu().tolist())
        if len(self.distance_history) > self.max_history_size:
            self.distance_history = self.distance_history[-self.max_history_size:]
        
        # è®¡ç®—è‡ªé€‚åº”é˜ˆå€¼
        if len(self.distance_history) > 100:
            max_distance = np.percentile(self.distance_history, self.percentile)
        else:
            max_distance = 10.0  # åˆå§‹å€¼
        
        # å½’ä¸€åŒ–
        rewards = 1.0 - torch.clamp(distances / max_distance, 0, 1)
        return rewards
```

### æ”¹è¿› 2: ä½¿ç”¨æŒ‡æ•°è¡°å‡ï¼ˆæ›´å¹³æ»‘ï¼‰

```python
class ExponentialMatchingScoreReward(MDMRewardFunction):
    def __init__(self, scale=2.0, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.scale = scale  # æ§åˆ¶è¡°å‡é€Ÿåº¦
    
    def __call__(self, motions, prompts, lengths=None):
        # ... è®¡ç®—è·ç¦» ...
        distances = torch.norm(text_embeddings - motion_embeddings, dim=-1)
        
        # æŒ‡æ•°è¡°å‡ï¼šè·ç¦»è¶Šå¤§ï¼Œå¥–åŠ±è¶Šå°
        rewards = torch.exp(-distances / self.scale)
        
        # å¯é€‰ï¼šå½’ä¸€åŒ–åˆ° [0, 1]
        # rewards = (rewards - rewards.min()) / (rewards.max() - rewards.min() + 1e-8)
        
        return rewards
```

### æ”¹è¿› 3: ç»„åˆå¥–åŠ±ï¼ˆè€ƒè™‘å¤šä¸ªç»´åº¦ï¼‰

```python
class MultiDimensionalReward(MDMRewardFunction):
    def __init__(
        self,
        matching_weight=0.6,
        smoothness_weight=0.2,
        diversity_weight=0.2,
        *args,
        **kwargs
    ):
        super().__init__(*args, **kwargs)
        self.matching_weight = matching_weight
        self.smoothness_weight = smoothness_weight
        self.diversity_weight = diversity_weight
    
    def _compute_smoothness(self, motions):
        """è®¡ç®—åŠ¨ä½œæµç•…æ€§ï¼ˆé€Ÿåº¦å˜åŒ–ï¼‰"""
        # è®¡ç®—é€Ÿåº¦
        velocities = motions[..., 1:] - motions[..., :-1]
        # è®¡ç®—åŠ é€Ÿåº¦
        accelerations = velocities[..., 1:] - velocities[..., :-1]
        # æµç•…æ€§ = è´Ÿçš„åŠ é€Ÿåº¦å˜åŒ–ï¼ˆå˜åŒ–è¶Šå°è¶Šæµç•…ï¼‰
        smoothness = -torch.norm(accelerations, dim=-1).mean(dim=-1)
        return smoothness
    
    def __call__(self, motions, prompts, lengths=None):
        # åŒ¹é…åº¦å¥–åŠ±
        matching_rewards = self._compute_matching(motions, prompts, lengths)
        
        # æµç•…æ€§å¥–åŠ±
        smoothness_rewards = self._compute_smoothness(motions)
        smoothness_rewards = torch.sigmoid(smoothness_rewards)  # å½’ä¸€åŒ–
        
        # ç»„åˆ
        rewards = (
            self.matching_weight * matching_rewards +
            self.smoothness_weight * smoothness_rewards
        )
        return rewards
```

### æ”¹è¿› 4: ç»„å†…å½’ä¸€åŒ–å¥–åŠ±ï¼ˆæ›´é€‚åˆ GRPOï¼‰

```python
class GroupNormalizedReward(MDMRewardFunction):
    """
    åœ¨ç»„å†…å½’ä¸€åŒ–å¥–åŠ±ï¼Œç¡®ä¿ç»„å†…å¥–åŠ±åˆ†å¸ƒåˆç†
    """
    def __call__(self, motions, prompts, lengths=None, group_size=None):
        # è®¡ç®—åŸå§‹å¥–åŠ±
        raw_rewards = self._compute_raw_rewards(motions, prompts, lengths)
        
        if group_size is not None:
            # åœ¨ç»„å†…å½’ä¸€åŒ–
            batch_size = raw_rewards.shape[0] // group_size
            rewards_reshaped = raw_rewards.view(batch_size, group_size)
            
            # ç»„å†…å½’ä¸€åŒ–åˆ° [0, 1]
            group_min = rewards_reshaped.min(dim=1, keepdim=True)[0]
            group_max = rewards_reshaped.max(dim=1, keepdim=True)[0]
            group_range = group_max - group_min + 1e-8
            
            normalized_rewards = (rewards_reshaped - group_min) / group_range
            return normalized_rewards.view(-1)
        else:
            return raw_rewards
```





# GRPO å¥–åŠ±å‡½æ•°ä½¿ç”¨

## æ¦‚è¿°

`train_grpo.py` ç°åœ¨æ”¯æŒä¸¤ç§å¥–åŠ±å‡½æ•°ï¼š

1. **MDM è¯„ä¼°å™¨å¥–åŠ±å‡½æ•°** (`reward_model.py`) - åŸºäº MDM é¡¹ç›®çš„è¯„ä¼°å™¨
2. **TMR é¢„è®­ç»ƒæ¨¡å‹å¥–åŠ±å‡½æ•°** (`reward_model_tmr.py`) - åŸºäº TMR é¢„è®­ç»ƒæƒé‡

## ä½¿ç”¨ç¤ºä¾‹(è§è®­ç»ƒä½¿ç”¨)

### 1. ä½¿ç”¨ MDM è¯„ä¼°å™¨å¥–åŠ±å‡½æ•°

### 2. ä½¿ç”¨ TMR é¢„è®­ç»ƒæ¨¡å‹å¥–åŠ±å‡½æ•°

#### 2.1 ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆæ¨èï¼‰

#### 2.2 ä½¿ç”¨åŒ¹é…åˆ†æ•°ï¼ˆå¯é…ç½®ï¼‰

## æ•…éšœæ’é™¤

### é—®é¢˜ 1: TMR æƒé‡æ–‡ä»¶æœªæ‰¾åˆ°

**é”™è¯¯ä¿¡æ¯**:

```
ValueError: ä½¿ç”¨ TMR å¥–åŠ±æ¨¡å‹æ—¶ï¼Œå¿…é¡»æä¾› --tmr_text_encoder_path å‚æ•°
ValueError: ä½¿ç”¨ TMR å¥–åŠ±æ¨¡å‹æ—¶ï¼Œå¿…é¡»æä¾› --tmr_motion_encoder_path å‚æ•°
ValueError: ä½¿ç”¨ TMR å¥–åŠ±æ¨¡å‹æ—¶ï¼Œå¿…é¡»æä¾› --tmr_movement_encoder_path å‚æ•°
```

**è§£å†³æ–¹æ¡ˆ**: ç¡®ä¿æä¾›äº†ä¸‰ä¸ªç‹¬ç«‹çš„æƒé‡æ–‡ä»¶è·¯å¾„ï¼š
- `--tmr_text_encoder_path`: text_encoder.pt
- `--tmr_motion_encoder_path`: motion_encoder.pt
- `--tmr_movement_encoder_path`: motion_decoder.pt

### é—®é¢˜ 2: TMR æƒé‡åŠ è½½å¤±è´¥

**é”™è¯¯ä¿¡æ¯**:
```
FileNotFoundError: TMR æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: ...
```

**è§£å†³æ–¹æ¡ˆ**: 
1. æ£€æŸ¥æƒé‡æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®
2. ç¡®ä¿æ–‡ä»¶æ ¼å¼æ­£ç¡®ï¼ˆ.pth æˆ– .tarï¼‰
3. å‚è€ƒ `TMR_REWARD_README.md` äº†è§£æƒé‡æ–‡ä»¶æ ¼å¼è¦æ±‚

### é—®é¢˜ 3: å¥–åŠ±å€¼å¼‚å¸¸

**é—®é¢˜**: å¥–åŠ±å€¼å…¨ä¸º 0 æˆ–å…¨ä¸º 1

**è§£å†³æ–¹æ¡ˆ**:
1. æ£€æŸ¥å¥–åŠ±å‡½æ•°æ˜¯å¦æ­£ç¡®åŠ è½½
2. å¯¹äº TMRï¼Œå°è¯•ä¸åŒçš„å½’ä¸€åŒ–æ–¹å¼
3. è°ƒæ•´ `--tmr_max_distance` æˆ– `--tmr_scale` å‚æ•°







# è®­ç»ƒä½¿ç”¨

### ä½¿ç”¨è®­ç»ƒè„šæœ¬()

- ä¸æŒ‚loraçš„çŠ¶æ€ä¸‹

  ```bash
  python -m train.train_grpo --model_path ./save/official_humanml_enc_512_50steps/model000750000.pt --save_dir ./save/grpo_finetuned --dataset humanml --batch_size 1 --group_size 4 --num_steps 10000 --learning_rate 1e-6 --reward_model_type mdm  --reward_type matching --device 0 
  ```

- æŒ‚loraçš„çŠ¶æ€

  ```bash
  python -m train.train_grpo --model_path ./save/pretrained_model/model000200000.pt --save_dir ./save/grpo_finetuned --dataset humanml --batch_size 1 --group_size 4 --num_steps 10000 --learning_rate 5e-7 --use_lora --lora_r 8 lora_alpha 16 --reward_model_type mdm \ --reward_type matching --device 0 
  ```

- ä½¿ç”¨mdmè¯„ä¼°å‡½æ•°ä½œä¸ºreward_model

  ```bash
  # ä½¿ç”¨åŒ¹é…åˆ†æ•°ï¼ˆé»˜è®¤ï¼‰
  python -m train.train_grpo --model_path ./save/official_humanml_enc_512_50steps/model000750000.pt --save_dir ./save/grpo_finetuned_humanml_enc_512_50steps_750000_mdm_matching --dataset humanml --batch_size 1 --group_size 4 --learning_rate 1e-6 --num_steps 15000 --reward_model_type mdm --reward_type matching --device 3
  
  # ä½¿ç”¨ R-Precision
  python -m train.train_grpo --model_path ./save/official_humanml_enc_512_50steps/model000750000.pt --save_dir ./save/grpo_finetuned_humanml_enc_512_50steps_750000_r_precision --dataset humanml --batch_size 1 --group_size 4 --learning_rate 1e-6 --num_steps 15000 --reward_model_type mdm --reward_type r_precision --device 4  
  
  # ä½¿ç”¨ç»„åˆå¥–åŠ±
  python -m train.train_grpo --model_path ./save/official_humanml_enc_512_50steps/model000750000.pt --save_dir ./save/grpo_finetuned_humanml_enc_512_50steps_750000_mdm_combined --dataset humanml --batch_size 1 --group_size 4 --learning_rate 1e-6 --num_steps 15000 --reward_model_type mdm  --reward_type combined  --device 1  
  ```

- ä½¿ç”¨ TMR é¢„è®­ç»ƒæ¨¡å‹å¥–åŠ±å‡½æ•°

  ä½™å¼¦ç›¸ä¼¼åº¦

  ```bash
  python -m train.train_grpo --model_path ./save/official_humanml_enc_512_50steps/model000750000.pt --save_dir ./save/grpo_finetuned_humanml_enc_512_50steps_750000_tmr_cosine --dataset humanml  --batch_size 1 --group_size 4 --learning_rate 1e-6  --num_steps 15000 --reward_model_type tmr --reward_type cosine --tmr_text_encoder_path ./model/GRPO/tmr_weights/text_encoder.pt --tmr_motion_encoder_path ./model/GRPO/tmr_weights/motion_encoder.pt --tmr_movement_encoder_path ./model/GRPO/tmr_weights/motion_decoder.pt --device 2
  ```

  åŒ¹é…åˆ†æ•°

  ```bash
  # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ + çº¿æ€§å½’ä¸€åŒ–
  python -m train.train_grpo --model_path ./save/official_humanml_enc_512_50steps/model000750000.pt --save_dir ./save/grpo_finetuned_humanml_enc_512_50steps_750000_tmr_matching_cosine --dataset humanml --batch_size 1 --group_size 4 --num_steps 10000 --reward_model_type tmr  --reward_type matching  --tmr_text_encoder_path ./model/GRPO/tmr_weights/text_encoder.pt --tmr_motion_encoder_path ./model/GRPO/tmr_weights/motion_encoder.pt --tmr_movement_encoder_path ./model/GRPO/tmr_weights/motion_decoder.pt  --tmr_similarity_type cosine --device 1
  
  # ä½¿ç”¨æ¬§æ°è·ç¦» + çº¿æ€§å½’ä¸€åŒ–
  python -m train.train_grpo --model_path ./save/official_humanml_enc_512_50steps/model000750000.pt  --save_dir ./save/grpo_finetuned_humanml_enc_512_50steps_750000_tmr_matching_euclidean_linear --dataset humanml --batch_size 1 --group_size 4 --num_steps 10000 --reward_model_type tmr --reward_type matching --tmr_text_encoder_path ./model/GRPO/tmr_weights/text_encoder.pt --tmr_motion_encoder_path ./model/GRPO/tmr_weights/motion_encoder.pt --tmr_movement_encoder_path ./model/GRPO/tmr_weights/motion_decoder.pt --tmr_similarity_type euclidean --tmr_normalization linear --tmr_max_distance 10.0 --device 0
  
  # ä½¿ç”¨æ¬§æ°è·ç¦» + æŒ‡æ•°å½’ä¸€åŒ–
  python -m train.train_grpo --model_path ./save/official_humanml_enc_512_50steps/model000750000.pt --save_dir ./save/grpo_finetuned_humanml_enc_512_50steps_750000_tmr_matching_euclidean_exp --dataset humanml --batch_size 1 --group_size 4 --num_steps 10000 --reward_model_type tmr --reward_type matching 
  --tmr_text_encoder_path ./model/GRPO/tmr_weights/text_encoder.pt --tmr_motion_encoder_path ./model/GRPO/tmr_weights/motion_encoder.pt --tmr_movement_encoder_path ./model/GRPO/tmr_weights/motion_decoder.pt --tmr_similarity_type euclidean --tmr_normalization exponential --tmr_scale 2.0 --device 1
  
  # ä½¿ç”¨æ¬§æ°è·ç¦» + Sigmoid å½’ä¸€åŒ–
  python -m train.train_grpo --model_path ./save/official_humanml_enc_512_50steps/model000750000.pt --save_dir /save/grpo_finetuned_humanml_enc_512_50steps_750000_tmr_matching_euclidean_sigmoid --dataset humanml --batch_size 1 --group_size 4 --num_steps 10000 --reward_model_type tmr --reward_type matching 
  --tmr_text_encoder_path ./model/GRPO/tmr_weights/text_encoder.pt --tmr_motion_encoder_path ./model/GRPO/tmr_weights/motion_encoder.pt --tmr_movement_encoder_path ./model/GRPO/tmr_weights/motion_decoder.pt --tmr_similarity_type euclidean --tmr_normalization sigmoid --tmr_scale 2.0 --device 2
  ```
  
  

**å¥–åŠ±ç±»å‹é€‰é¡¹**ï¼š
- `--reward_model_type`: é€‰æ‹©å¥–åŠ±æ¨¡å‹ç±»å‹
  - `mdm` (é»˜è®¤): ä½¿ç”¨ MDM è¯„ä¼°å™¨å¥–åŠ±å‡½æ•°
  - `tmr`: ä½¿ç”¨ TMR é¢„è®­ç»ƒæ¨¡å‹å¥–åŠ±å‡½æ•°

- `--reward_type`: å¥–åŠ±ç±»å‹
  - å¯¹äº MDM: `matching`, `r_precision`, `combined`
  - å¯¹äº TMR: `matching`, `cosine`

### MDM å¥–åŠ±å‡½æ•°å‚æ•°

| å‚æ•°                  | é€‰é¡¹          | è¯´æ˜                              |
| --------------------- | ------------- | --------------------------------- |
| `--reward_model_type` | `mdm`         | ä½¿ç”¨ MDM è¯„ä¼°å™¨                   |
| `--reward_type`       | `matching`    | åŸºäºæ–‡æœ¬-åŠ¨ä½œåŒ¹é…åˆ†æ•°ï¼ˆæ¬§æ°è·ç¦»ï¼‰ |
|                       | `r_precision` | åŸºäº R-Precision æ£€ç´¢ç²¾åº¦         |
|                       | `combined`    | ç»„åˆåŒ¹é…åˆ†æ•°å’Œ R-Precision        |

### TMR å¥–åŠ±å‡½æ•°å‚æ•°

| å‚æ•°                          | é€‰é¡¹          | è¯´æ˜                                             |
| ----------------------------- | ------------- | ------------------------------------------------ |
| `--reward_model_type`         | `tmr`         | ä½¿ç”¨ TMR é¢„è®­ç»ƒæ¨¡å‹                              |
| `--reward_type`               | `cosine`      | ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆæœ€ç®€å•ï¼Œæ¨èï¼‰                       |
|                               | `matching`    | åŒ¹é…åˆ†æ•°ï¼ˆå¯é…ç½®ç›¸ä¼¼åº¦å’Œå½’ä¸€åŒ–ï¼‰                 |
| `--tmr_text_encoder_path`     | è·¯å¾„          | TMR æ–‡æœ¬ç¼–ç å™¨æƒé‡è·¯å¾„ (text_encoder.ptï¼Œå¿…éœ€)   |
| `--tmr_motion_encoder_path`   | è·¯å¾„          | TMR åŠ¨ä½œç¼–ç å™¨æƒé‡è·¯å¾„ (motion_encoder.ptï¼Œå¿…éœ€) |
| `--tmr_movement_encoder_path` | è·¯å¾„          | TMR åŠ¨ä½œè§£ç å™¨æƒé‡è·¯å¾„ (motion_decoder.ptï¼Œå¿…éœ€) |
| `--tmr_similarity_type`       | `cosine`      | ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆæ¨èï¼‰                               |
|                               | `euclidean`   | æ¬§æ°è·ç¦»                                         |
| `--tmr_normalization`         | `linear`      | çº¿æ€§å½’ä¸€åŒ–                                       |
|                               | `exponential` | æŒ‡æ•°è¡°å‡å½’ä¸€åŒ–                                   |
|                               | `sigmoid`     | Sigmoid å½’ä¸€åŒ–                                   |
| `--tmr_max_distance`          | æµ®ç‚¹æ•°        | æœ€å¤§è·ç¦»ï¼ˆç”¨äºçº¿æ€§å½’ä¸€åŒ–ï¼Œé»˜è®¤: 10.0ï¼‰           |
| `--tmr_scale`                 | æµ®ç‚¹æ•°        | ç¼©æ”¾å› å­ï¼ˆç”¨äºæŒ‡æ•°/Sigmoidï¼Œé»˜è®¤: 2.0ï¼‰          |

**TMRç‰¹å®šå‚æ•°**

- `--tmr_text_encoder_path`: TMR æ–‡æœ¬ç¼–ç å™¨æƒé‡è·¯å¾„ (text_encoder.ptï¼Œå¿…éœ€)
- `--tmr_motion_encoder_path`: TMR åŠ¨ä½œç¼–ç å™¨æƒé‡è·¯å¾„ (motion_encoder.ptï¼Œå¿…éœ€)
- `--tmr_movement_encoder_path`: TMR åŠ¨ä½œè§£ç å™¨æƒé‡è·¯å¾„ (motion_decoder.ptï¼Œå¿…éœ€)
- `--tmr_similarity_type`: ç›¸ä¼¼åº¦ç±»å‹ (`cosine` æˆ– `euclidean`ï¼Œé»˜è®¤: `cosine`)
- `--tmr_normalization`: å½’ä¸€åŒ–æ–¹å¼ (`linear`, `exponential`, `sigmoid`ï¼Œé»˜è®¤: `linear`)
- `--tmr_max_distance`: æœ€å¤§è·ç¦»ï¼ˆç”¨äºçº¿æ€§å½’ä¸€åŒ–ï¼Œé»˜è®¤: `10.0`ï¼‰
- `--tmr_scale`: ç¼©æ”¾å› å­ï¼ˆç”¨äºæŒ‡æ•°/Sigmoid å½’ä¸€åŒ–ï¼Œé»˜è®¤: `2.0`ï¼‰

## æ³¨æ„äº‹é¡¹

1. **TMR æƒé‡æ–‡ä»¶**: ä½¿ç”¨ TMR å¥–åŠ±å‡½æ•°æ—¶ï¼Œå¿…é¡»æä¾›ä¸‰ä¸ªç‹¬ç«‹çš„æƒé‡æ–‡ä»¶è·¯å¾„ï¼š
   - `--tmr_text_encoder_path`: æ–‡æœ¬ç¼–ç å™¨æƒé‡ (text_encoder.pt)
   - `--tmr_motion_encoder_path`: åŠ¨ä½œç¼–ç å™¨æƒé‡ (motion_encoder.pt)
   - `--tmr_movement_encoder_path`: åŠ¨ä½œè§£ç å™¨æƒé‡ (motion_decoder.pt)
2. **å‚æ•°å…¼å®¹æ€§**: 
   - `--tmr_similarity_type`, `--tmr_normalization` ç­‰å‚æ•°ä»…åœ¨ `--reward_type=matching` æ—¶ç”Ÿæ•ˆ
   - å½“ `--reward_type=cosine` æ—¶ï¼Œè¿™äº›å‚æ•°ä¼šè¢«å¿½ç•¥
3. **æ•°æ®é›†æ”¯æŒ**: ä¸¤ç§å¥–åŠ±å‡½æ•°éƒ½æ”¯æŒ `humanml` å’Œ `kit` æ•°æ®é›†
4. **æ€§èƒ½**: 
   - MDM è¯„ä¼°å™¨ï¼šä½¿ç”¨é¡¹ç›®å†…ç½®çš„è¯„ä¼°å™¨ï¼Œæ— éœ€é¢å¤–ä¸‹è½½
   - TMRï¼šéœ€è¦ä¸‹è½½ä¸‰ä¸ªé¢„è®­ç»ƒæƒé‡æ–‡ä»¶ï¼Œä½†å¯èƒ½æä¾›æ›´å¥½çš„æ–‡æœ¬-åŠ¨ä½œå¯¹é½

**å¯è§†åŒ–å¹³å°é€‰é¡¹**ï¼š

- `--train_platform_type NoPlatform`: ä¸ä½¿ç”¨å¯è§†åŒ–ï¼ˆé»˜è®¤ï¼‰
- `--train_platform_type TensorboardPlatform`: ä½¿ç”¨ TensorBoard
- `--train_platform_type WandBPlatform`: ä½¿ç”¨ Weights & Biases
- `--train_platform_type ClearmlPlatform`: ä½¿ç”¨ ClearML

**ä½¿ç”¨ TensorBoard æŸ¥çœ‹è®­ç»ƒè¿›åº¦**ï¼š

```bash
# è®­ç»ƒæ—¶ä½¿ç”¨ TensorBoard
python -m train.train_grpo ... --train_platform_type TensorboardPlatform

# åœ¨å¦ä¸€ä¸ªç»ˆç«¯å¯åŠ¨ TensorBoard
tensorboard --logdir ./save/grpo_finetuned
```

**è®°å½•çš„è®­ç»ƒæŒ‡æ ‡**ï¼š

- **Loss**: `loss`, `policy_loss`, `kl_penalty`
- **Reward**: `mean_reward`, `std_reward`, `min_reward`, `max_reward`
- **Advantage**: `mean_advantage`, `std_advantage`
- **LogProb**: `mean_log_prob_current`, `mean_log_prob_ref`, `mean_ratio`
- **Training**: `grad_norm`, `learning_rate`

- è¯„ä¼°å‘½ä»¤
python -m eval.eval_humanml --model_path ./save/humanml_trans_enc_512/model000475000.pt